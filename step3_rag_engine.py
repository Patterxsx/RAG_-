import os
import torch
from typing import List, Dict
from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
import json

class HongLouRAG:
    def __init__(self):
        print("ğŸ‹ æ­£åœ¨åˆå§‹åŒ–çº¢æ¥¼RAGç³»ç»Ÿ...")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        print("ğŸ“š åŠ è½½çŸ¥è¯†åº“...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-large-zh-v1.5",
            model_kwargs={'device': 'cpu'}
        )
        self.vectorstore = FAISS.load_local(
            "faiss_index/honglou_index",
            self.embeddings,
            allow_dangerous_deserialization=True
        )
        print(f"âœ… å·²åŠ è½½ {self.vectorstore.index.ntotal} æ¡çŸ¥è¯†")
    
        print("ğŸ§  åŠ è½½Qwen2.5-7Bï¼ˆ4-bité‡åŒ–ï¼Œçº¦4.5GBæ˜¾å­˜ï¼‰...")
        self._load_llm()
        
        self._init_prompts()
        
        print("âœ¨ ç³»ç»Ÿå°±ç»ªï¼æ˜¾å­˜å ç”¨:", self._get_gpu_memory(), "MB")
    
    def _load_llm(self):
        model_name = "qwen/Qwen2.5-7B-Instruct-GPTQ-Int4"
        
        print(f"  æ­£åœ¨ä¸‹è½½/åŠ è½½ {model_name}...")
        model_dir = snapshot_download(model_name)
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_dir,
            trust_remote_code=True,
            pad_token='<|im_end|>',
            padding_side='left'
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_dir,
            device_map="auto",         
            trust_remote_code=True,
            torch_dtype=torch.float16,   
        )
        
        self.llm = pipeline(
            "text-generation",
            model=model,
            tokenizer=self.tokenizer,
            max_new_tokens=1024,         
            temperature=0.3,           
            top_p=0.9,
            repetition_penalty=1.05,
            do_sample=True,
            return_full_text=False       
        )
        print(" æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def _init_prompts(self):
        """åˆå§‹åŒ–çº¢å­¦ä¸“å®¶Prompt"""
        
        self.rag_template = """ä½ æ˜¯ä¸€ä½ç²¾é€šã€Šçº¢æ¥¼æ¢¦ã€‹çš„èµ„æ·±çº¢å­¦ç ”ç©¶è€…ï¼Œæ“…é•¿æ–‡æœ¬ç»†è¯»ä¸è„‚ç šæ–‹è¯„æ³¨è§£è¯»ã€‚

ã€ä»»åŠ¡ã€‘åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„åŸæ–‡ä¸è¯„æ³¨ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å›ç­”è¦æ±‚ï¼š
1. å¿…é¡»å¼•ç”¨å…·ä½“å›ç›®ï¼ˆå¦‚"è§äºç¬¬ä¸‰å›"ï¼‰
2. ç»“åˆè„‚ç šæ–‹è¯„æ³¨ï¼ˆå¦‚æœ‰ï¼‰åˆ†ææ·±å±‚å«ä¹‰
3. æŒ‡å‡ºè‰ºæœ¯æ‰‹æ³•ï¼ˆè‰è›‡ç°çº¿ã€æ˜¥ç§‹ç¬”æ³•ç­‰ï¼‰

ã€æ£€ç´¢èµ„æ–™ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€çº¢å­¦åˆ†æã€‘"""
    
    def _get_gpu_memory(self):

        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / 1024**2
        return 0
    
    def retrieve(self, query: str, k: int = 4) -> List[Document]:
        """æ£€ç´¢ç›¸å…³æ®µè½"""
        print(f"ğŸ” æ£€ç´¢: '{query}'...")
        docs = self.vectorstore.similarity_search(query, k=k)
        
        seen_chapters = set()
        unique_docs = []
        for doc in docs:
            ch = doc.metadata.get('chapter', 0)
            if ch not in seen_chapters:
                seen_chapters.add(ch)
                unique_docs.append(doc)
        
        return unique_docs[:3]  # å–å‰3ä¸ªä¸åŒå›ç›®
    
    def generate(self, query: str) -> Dict:
        """ç”Ÿæˆå›ç­”"""
        docs = self.retrieve(query)

        context_parts = []
        for i, doc in enumerate(docs, 1):
            source = doc.metadata.get('type', 'æ­£æ–‡')
            chapter = doc.metadata.get('chapter_title', 'unknown')
            content = doc.page_content[:300] 
            
            context_parts.append(
                f"[{i}] {chapter}ï¼ˆ{source}ï¼‰ï¼š{content}..."
            )
        
        context = "\n\n".join(context_parts)
        
        prompt = self.rag_template.format(
            context=context,
            question=query
        )
        
        print("ğŸ“ ç”Ÿæˆå›ç­”ä¸­...")
        try:
            response = self.llm(prompt)[0]['generated_text']
        
            if prompt in response:
                response = response.replace(prompt, "").strip()
            
            return {
                "query": query,
                "answer": response,
                "sources": docs,
                "context": context
            }
        except Exception as e:
            print(f"ç”Ÿæˆé”™è¯¯: {e}")
            return {"error": str(e)}
    
    def chat(self):
        """äº¤äº’å¼å¯¹è¯"""
        print("\n" + "="*60)
        print("çº¢æ¥¼RAG")
        print("="*60)
        print("è¾“å…¥é—®é¢˜ï¼Œè¾“å…¥'quit'é€€å‡º")
        print("="*60 + "\n")
        
        while True:
            try:
                query = input("æ‚¨çš„é—®é¢˜ï¼š").strip()
                if not query:
                    continue
                if query.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("å†è§ï¼")
                    break
                result = self.generate(query)
                if "error" in result:
                    print(f"é”™è¯¯: {result['error']}")
                    continue
                print("\n" + "-"*60)
                print("ğŸ“– å¼•ç”¨èµ„æ–™ï¼š")
                for doc in result['sources']:
                    ch = doc.metadata.get('chapter_title', 'æœªçŸ¥')
                    t = doc.metadata.get('type', 'æ­£æ–‡')
                    print(f"   â€¢ {ch}ï¼ˆ{t}ï¼‰")
                
                print("\nğŸ’¬ ä¸“å®¶è§£è¯»ï¼š")
                print(result['answer'])
                print("-"*60 + "\n")
            
                if self.device == "cuda":
                    print(f"[æ˜¾å­˜å ç”¨: {self._get_gpu_memory():.1f}MB]")
                
            except KeyboardInterrupt:
                print("\nå†è§ï¼")
                break
            except Exception as e:
                print(f"é”™è¯¯: {e}")

def main():
    rag = HongLouRAG()
    rag.chat()

if __name__ == "__main__":
    main()