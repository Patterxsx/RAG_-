# -*- coding: utf-8 -*-
import os
import re
import json
from pathlib import Path
from typing import List, Dict
from tqdm import tqdm

def extract_honglou(tex_folder: str, output_file: str = "honglou_data.jsonl"):
    """
    ä»LaTeXè„‚è¯„æœ¬ä¸­æå–ç»“æ„åŒ–æ•°æ®
    """
    tex_path = Path(tex_folder)
    tex_files = sorted(tex_path.glob("*.tex"))
    print(f"âœ… å‘ç° {len(tex_files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
    all_documents = []
    for tex_file in tqdm(tex_files, desc="è§£ææ–‡ä»¶"):
        docs = parse_single_file(tex_file)
        all_documents.extend(docs)
    with open(output_file, 'w', encoding='utf-8') as f:       # ä¿å­˜ä¸ºJSONLï¼ˆRAGæ ‡å‡†æ ¼å¼ï¼‰
        for doc in all_documents:
            f.write(json.dumps(doc, ensure_ascii=False) + '\n')
    generate_report(all_documents, output_file)

def parse_single_file(file_path: Path) -> List[Dict]:
    """è§£æå•ä¸ªtexæ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    chapter_info = extract_chapter_info(content, file_path.name)
    raw_main_text, comments = split_content_and_comments(content)      # åˆ†ç¦»æ­£æ–‡å’Œè„‚è¯„
    main_text = clean_latex_content(raw_main_text)
    documents = []
    #  å¤„ç†æ­£æ–‡ï¼šæŒ‰æ®µè½åˆ‡åˆ†
    paragraphs = split_paragraphs(main_text)
    for idx, para in enumerate(paragraphs):
        if len(para.strip()) < 20:  # è¿‡æ»¤å¤ªçŸ­çš„
            continue
        documents.append({
            "id": f"{chapter_info['num']:03d}-{idx:03d}",
            "content": para.strip(),
            "metadata": {
                "chapter": chapter_info['num'],
                "chapter_title": chapter_info['title'],
                "type": "æ­£æ–‡",
                "source_file": file_path.name
            }
        })
    #  å¤„ç†è„‚è¯„
    for idx, comment in enumerate(comments):
        if len(comment.strip()) < 5:
            continue
        documents.append({
            "id": f"{chapter_info['num']:03d}-z-{idx:03d}",
            "content": comment.strip(),
            "metadata": {
                "chapter": chapter_info['num'],
                "chapter_title": chapter_info['title'],
                "type": "è„‚è¯„",
                "source_file": file_path.name
            }
        })
    
    return documents

def extract_chapter_info(content: str, filename: str) -> Dict:
    """æå–ç¬¬Xå›ä¿¡æ¯"""
    # ä»æ–‡ä»¶åæå–
    num_match = re.search(r'chapter(\d+)', filename)
    if num_match:
        num = int(num_match.group(1))
        lines = content.split('\n')[:10]
        for line in lines:
            if 'ç¬¬' in line and 'å›' in line:
                return {"num": num, "title": line.strip()}
        return {"num": num, "title": f"ç¬¬{num}å›"}
    
    return {"num": 0, "title": "æœªçŸ¥å›ç›®"}

def clean_latex_content(text: str) -> str:
    """æ¸…ç†LaTeXæ ‡è®°ï¼Œä¿ç•™æ–‡æœ¬å†…å®¹"""
    # å»é™¤æ–‡æ¡£ç±»å®šä¹‰ç­‰å¤´éƒ¨
    text = re.sub(r'\\documentclass[^}]*\}', '', text)
    text = re.sub(r'\\usepackage[^}]*\}', '', text)
    text = re.sub(r'\\begin\{document\}', '', text)
    text = re.sub(r'\\end\{document\}', '', text)
    # å»é™¤æ³¨é‡Šï¼ˆ%å¼€å¤´ï¼‰
    text = re.sub(r'^\s*%.*$', '', text, flags=re.MULTILINE)
    # æå–å‘½ä»¤å‚æ•°ï¼š\command{arg} -> arg
    # é‡å¤å¤šæ¬¡å¤„ç†åµŒå¥—
    for _ in range(3):
        text = re.sub(r'\\[a-zA-Z]+\*?\{([^}]*)\}', r'\1', text)
        text = re.sub(r'\\[a-zA-Z]+\*?\[([^\]]*)\]', r'\1', text)
    # å»é™¤å‰©ä½™å‘½ä»¤
    text = re.sub(r'\\[a-zA-Z]+', '', text)
    # æ¸…ç†ç‰¹æ®Šç¬¦å·
    text = text.replace('\\', '')
    text = text.replace('&', '')
    text = text.replace('#', '')
    text = text.replace('_', '')
    # è§„èŒƒåŒ–ç©ºç™½
    text = re.sub(r'\s+', '\n', text)
    text = re.sub(r'\.5em', '', text)
    text = re.sub(r'\n\s*\n', '\n\n', text)

    # æ¸…ç†æ®‹ç•™çš„ includegraphics å‚æ•°å—ï¼š{width=3mm{../Images/00004}
    text = re.sub(r'\{width=3mm(?:\{[^}]+\})+\}?', '', text)
    # æ¸…ç†æ®‹ç•™çš„ Images è·¯å¾„ï¼ˆå¯èƒ½å•ç‹¬æ®‹ç•™ï¼‰
    text = re.sub(r'\{[^}]*Images/[^}]+\}\}?', '', text)
    # æ¸…ç†æ®‹ç•™çš„æ•°å­¦å…¬å¼æ ‡è®°ï¼ˆå¦‚æ ·æœ¬ä¸­çš„{$$}ï¼‰
    text = re.sub(r'\{\$\$?\}', '', text)
    text = re.sub(r'width=3mm', '', text)
        # æ¸…ç†å¤¹æ‰¹åˆ é™¤åæ®‹ç•™çš„å­¤ç«‹ }ï¼ˆé€šå¸¸åœ¨å¥æœ«æˆ–æ®µæœ«ï¼‰
    text = re.sub(r'([ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼šã€"â€â€™])\s*\}\s*', r'\1', text)  # æ ‡ç‚¹åçš„ }
    text = re.sub(r'\s*\}\s*([ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼šã€"â€â€™])', r'\1', text)  # æ ‡ç‚¹å‰çš„ }
    text = re.sub(r'^\s*\}\s*', '', text, flags=re.MULTILINE)     # è¡Œé¦–çš„ }
    
    # æ¸…ç†LaTeXæ–¹æ‹¬å·æ ‡è®°ï¼š{[}ä¹‹{]} -> ä¹‹ï¼ˆä¿ç•™å†…å®¹ï¼Œåˆ é™¤æ ‡è®°ï¼‰
    text = re.sub(r'\{\[\}', '', text)  # åˆ é™¤ {[
    text = re.sub(r'\{\]\}', '', text)  # åˆ é™¤ ]}
    
    # æ¸…ç†è¿ç»­å¤šä½™çš„ç©ºæ ¼
    text = re.sub(r' +', ' ', text)
    return text.strip()

def split_content_and_comments(text: str) -> tuple:
    """
    åˆ†ç¦»æ­£æ–‡å’Œè„‚è¯„
    è„‚è¯„å¸¸è§æ ¼å¼ï¼šã€æ‰¹è¯­å†…å®¹ã€‘æˆ–(çœ‰æ‰¹ï¼šå†…å®¹)æˆ–\footnote{å†…å®¹}
    """
    comments = []
    
    footnote_pattern = r'\\footnote\{((?:[^{}]|\{[^{}]*\})*)\}'
    for match in re.findall(footnote_pattern, text):
        # æ¸…ç†è„šæ³¨å†…çš„LaTeXæ ‡è®°ï¼ˆå¦‚{[}ä¹‹{]} -> ä¹‹ï¼‰
        clean = re.sub(r'\{([^{}]*)\}', r'\1', match)
        clean = re.sub(r'\\[a-zA-Z]+', '', clean)  # åˆ é™¤å‰©ä½™å‘½ä»¤
        comments.append(f"[å›æ‰¹] {clean.strip()}")
    text = re.sub(footnote_pattern, '', text)
    
    kaishu_pattern = r'\{\\includegraphics[^{}]*(?:\{[^}]*\})*\s*\\kaishu\s+((?:[^{}]|\{[^{}]*\})+?)\s*\}'
    for match in re.findall(kaishu_pattern, text):
        # æ¸…ç† {[}ä¹‹{]} æˆ– {$\diamond$} ç­‰
        clean = re.sub(r'\{([^{}]*)\}', r'\1', match)  # å»ä¸€å±‚{}
        clean = re.sub(r'\\[a-zA-Z]+', '', clean)      # å»å‘½ä»¤å¦‚\diamond
        clean = re.sub(r'\$\$\s*\$', '', clean)        # å»$$å’Œ$
        if len(clean.strip()) > 3:  # è¿‡æ»¤å¤ªçŸ­çš„
            comments.append(f"[å¤¹æ‰¹] {clean.strip()}")    # åˆ é™¤æ•´ä¸ª includegraphics+kaishu ç»„ï¼ˆé¿å…æ±¡æŸ“æ­£æ–‡ï¼‰
    text = re.sub(r'\{\\includegraphics[^{}]*(?:\{[^}]*\})*\}', '', text)
    return text, comments

def split_paragraphs(text: str) -> List[str]:
    """æ™ºèƒ½åˆ†æ®µï¼šæŒ‰ç©ºè¡Œåˆ†ï¼Œä½†åˆå¹¶çŸ­è¡Œ"""
    lines = text.split('\n')
    paragraphs = []
    current = ""
    
    for line in lines:
        line = line.strip()
        if not line:
            if current:
                paragraphs.append(current)
                current = ""
        else:
            # å¦‚æœå½“å‰æ®µè½å¾ˆé•¿ä¸”ä»¥å¥å·ç»“æŸï¼Œå…ˆå­˜èµ·æ¥
            if len(current) > 200 and current.endswith('ã€‚'):
                paragraphs.append(current)
                current = line
            else:
                current += line
    
    if current:
        paragraphs.append(current)
    
    return paragraphs

def chinese_to_number(chinese: str) -> int:
    """ä¸­æ–‡æ•°å­—è½¬é˜¿æ‹‰ä¼¯æ•°å­—ï¼ˆæ”¯æŒåˆ°åƒï¼‰"""
    num_map = {'ä¸€':1, 'äºŒ':2, 'ä¸‰':3, 'å››':4, 'äº”':5, 
               'å…­':6, 'ä¸ƒ':7, 'å…«':8, 'ä¹':9, 'å':10,
               'ç™¾':100, 'åƒ':1000, 'é›¶':0, 'ã€‡':0}
    
    result = 0
    temp = 0
    for char in chinese:
        if char in num_map:
            n = num_map[char]
            if n >= 10:
                if temp == 0:
                    temp = 1
                result += temp * n
                temp = 0
            else:
                temp = temp * 10 + n if temp > 0 else n
    result += temp
    return result if result > 0 else 0

def generate_report(docs: List[Dict], output_file: str):
    """ç”Ÿæˆæå–æŠ¥å‘Š"""
    total = len(docs)
    main_count = len([d for d in docs if d['metadata']['type'] == 'æ­£æ–‡'])
    comment_count = len([d for d in docs if d['metadata']['type'] == 'è„‚è¯„'])
    chapters = len(set(d['metadata']['chapter'] for d in docs))
    
    print(f"\n{'='*50}")
    print(f"âœ… æå–å®Œæˆï¼")
    print(f"{'='*50}")
    print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶ï¼š{output_file}")
    print(f"ğŸ“š è¦†ç›–å›ç›®ï¼š{chapters} å›")
    print(f"ğŸ“ æ­£æ–‡æ®µè½ï¼š{main_count} ä¸ª")
    print(f"ğŸ’¬ è„‚è¯„æ¡ç›®ï¼š{comment_count} æ¡")
    print(f"ğŸ“Š æ€»è®¡æ–‡æ¡£ï¼š{total} æ¡")
    print(f"{'='*50}")
    
    # æ˜¾ç¤ºæ ·æœ¬
    print(f"\nğŸ“ æ•°æ®æ ·æœ¬ï¼š")
    for i, doc in enumerate(docs[:3]):
        print(f"\n{i+1}. [{doc['metadata']['type']}] {doc['metadata']['chapter_title']}")
        print(f"   å†…å®¹ï¼š{doc['content'][:-1]}...")

if __name__ == "__main__":
    # äº¤äº’å¼è¾“å…¥
    folder = input("ğŸ“‚ è¯·è¾“å…¥LaTeXæ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„ï¼š").strip().strip('"').strip("'")
    output = input("ğŸ’¾ è¾“å‡ºæ–‡ä»¶åï¼ˆé»˜è®¤ï¼šhonglou_data.jsonlï¼‰ï¼š").strip()
    if not output:
        output = "honglou_data.jsonl"
    
    extract_honglou(folder, output)
