import json
import os
from typing import List, Dict
from tqdm import tqdm
import numpy as np

from langchain_core.documents import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

class HongLouVectorizer:
    def __init__(self, jsonl_path: str = "honglou_data.jsonl"):
        self.jsonl_path = jsonl_path
        self.output_dir = "faiss_index"
        print("æ­£åœ¨åŠ è½½Embeddingæ¨¡å‹ï¼ˆçº¦1GBå†…å­˜ï¼‰...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-large-zh-v1.5",  
            model_kwargs={'device': 'cpu'},       
            encode_kwargs={
                'normalize_embeddings': True,     
                'batch_size': 64                  
            }
        )
        print("æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def load_data(self) -> List[Document]:
        """åŠ è½½ç¬¬ä¸€æ­¥ç”Ÿæˆçš„æ•°æ®"""
        print(f"æ­£åœ¨åŠ è½½ {self.jsonl_path}...")
        documents = []
        
        with open(self.jsonl_path, 'r', encoding='utf-8') as f:
            for line in tqdm(f, desc="è¯»å–æ•°æ®"):
                data = json.loads(line.strip())

                content = data['content']
                if data['metadata']['type'] == 'è„‚è¯„':
                    content = f"[è„‚è¯„] {content}"
                
                doc = Document(
                    page_content=content,
                    metadata=data['metadata']
                )
                documents.append(doc)
        
        print(f"å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        print(f"  - æ­£æ–‡: {len([d for d in documents if 'è„‚è¯„' not in d.page_content])}")
        print(f"  - è„‚è¯„: {len([d for d in documents if 'è„‚è¯„' in d.page_content])}")
        return documents
    
    def build_index(self, documents: List[Document]):
        """æ„å»ºFAISSå‘é‡åº“"""
        os.makedirs(self.output_dir, exist_ok=True)
        
        print("å¼€å§‹æ„å»ºå‘é‡ç´¢å¼•ï¼ˆçº¦éœ€2-5åˆ†é’Ÿï¼‰...")
        batch_size = 500
        vectorstore = None
        
        for i in tqdm(range(0, len(documents), batch_size), desc="æ„å»ºç´¢å¼•"):
            batch = documents[i:i+batch_size]
            
            if vectorstore is None:
                vectorstore = FAISS.from_documents(
                    batch, 
                    self.embeddings,
                    distance_strategy="COSINE" 
                )
            else:
   
                vectorstore.add_documents(batch)
        
        index_path = os.path.join(self.output_dir, "honglou_index")
        vectorstore.save_local(index_path)
        
        print(f"\nâœ… ç´¢å¼•æ„å»ºå®Œæˆï¼")
        print(f"ä¿å­˜ä½ç½®: {index_path}/")
        print(f"åŒ…å«æ–‡ä»¶: index.faiss (å‘é‡æ•°æ®) + index.pkl (metadataæ˜ å°„)")
        
        return vectorstore
    
    def verify_index(self, vectorstore):
        """éªŒè¯æµ‹è¯•ï¼šç¡®ä¿æ£€ç´¢æ­£å¸¸å·¥ä½œ"""
        print("\n" + "="*50)
        print("ğŸ” ç´¢å¼•éªŒè¯æµ‹è¯•")
        print("="*50)
        
        test_queries = [
            "æ—é»›ç‰è¿›è´¾åºœ",
            "å®ç‰å’Œé»›ç‰çš„æ„Ÿæƒ…",
            "ç‹ç†™å‡¤çš„ç®¡ç†æ‰‹æ®µ",
            "å…ƒæ˜¥çœäº²"
        ]
        
        for query in test_queries:
            print(f"\næŸ¥è¯¢: '{query}'")
            results = vectorstore.similarity_search(query, k=2)
            
            for i, doc in enumerate(results, 1):
                source = doc.metadata.get('chapter_title', 'æœªçŸ¥')
                doc_type = doc.metadata.get('type', 'æœªçŸ¥')
                preview = doc.page_content[:40].replace('\n', '')
                print(f"  {i}. [{source}-{doc_type}] {preview}...")
        
        print("\nâœ… éªŒè¯é€šè¿‡ï¼ç´¢å¼•å¯æ­£å¸¸å·¥ä½œ")

def main():
    vectorizer = HongLouVectorizer()
    
    docs = vectorizer.load_data()
    
    vectorstore = vectorizer.build_index(docs)
    
    vectorizer.verify_index(vectorstore)

if __name__ == "__main__":
    main()